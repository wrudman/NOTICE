{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11943da7-6d6d-4661-8b9d-4df6fd7f205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# This file is provided in the Github of SVO-Probes (https://github.com/google-deepmind/svo_probes)\n",
    "df=pd.read_csv(\"svo_probes.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2cab19-cdc2-428a-853a-cc04a2ef6ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def parse_triplet(triplet_str):\n",
    "    try:\n",
    "        # Attempt to parse it as a literal list; use the first element if it's a list\n",
    "        parsed = ast.literal_eval(triplet_str)\n",
    "        if isinstance(parsed, list) and len(parsed) > 0:\n",
    "            return parsed[0].split(',')\n",
    "        else:\n",
    "            return triplet_str.split(',')\n",
    "    except:\n",
    "        # Fallback for non-list strings or if ast.literal_eval fails\n",
    "        return triplet_str.split(',')\n",
    "\n",
    "def generate_fixed_prompt(row):\n",
    "    # Function to parse triplets that might be stored as strings representing lists\n",
    "    def parse_triplet(triplet_str):\n",
    "        try:\n",
    "            # Attempt to parse it as a literal list; use the first element if it's a list\n",
    "            parsed = ast.literal_eval(triplet_str)\n",
    "            if isinstance(parsed, list) and len(parsed) > 0:\n",
    "                return parsed[0].split(',')\n",
    "            else:\n",
    "                raise ValueError(\"Triplet is not a list or is empty\")\n",
    "        except:\n",
    "            # Fallback for non-list strings or if ast.literal_eval fails\n",
    "            return triplet_str.split(',')\n",
    "\n",
    "    try:\n",
    "        pos_triplet = parse_triplet(row['pos_triplet'])\n",
    "        neg_triplet = parse_triplet(row['neg_triplet'])\n",
    "        \n",
    "        if len(pos_triplet) != 3 or len(neg_triplet) != 3:\n",
    "            raise ValueError(\"Triplet does not contain exactly 3 elements\")\n",
    "\n",
    "        # Handling each negation case\n",
    "        if row['subj_neg']:\n",
    "            prompt = f\"is {pos_triplet[0]} or {neg_triplet[0]} {pos_triplet[1]} {pos_triplet[2]}?\"\n",
    "        elif row['verb_neg']:\n",
    "            prompt = f\"is {pos_triplet[0]} {pos_triplet[1]} or {neg_triplet[1]} {pos_triplet[2]}?\"\n",
    "        elif row['obj_neg']:\n",
    "            prompt = f\"is {pos_triplet[0]} {pos_triplet[1]} {pos_triplet[2]} or {neg_triplet[2]}?\"\n",
    "        else:\n",
    "            prompt = f\"is {pos_triplet[0]} {pos_triplet[1]} {pos_triplet[2]}?\"\n",
    "    except Exception as e:\n",
    "        prompt = \"Error in triplet format: \" + str(e)\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Assuming df is already defined and has the necessary columns\n",
    "df['clean_prompt'] = df.apply(generate_fixed_prompt, axis=1)\n",
    "df['clean_prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955357cd-7a55-43d2-be78-cd80c77892d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_bad_triplets(df):\n",
    "    def is_bad_triplet(triplet_str):\n",
    "        try:\n",
    "            # Attempt to parse it as a literal list\n",
    "            parsed = ast.literal_eval(triplet_str)\n",
    "            # Check if parsed is an empty list or contains an empty list\n",
    "            if not parsed or parsed == ['[]']:\n",
    "                return True\n",
    "        except:\n",
    "            # Direct string comparison to catch malformed strings\n",
    "            if triplet_str == \"['[]']\":\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    # Filter out rows where either pos_triplet or neg_triplet is bad\n",
    "    return df[~df['pos_triplet'].apply(is_bad_triplet) & ~df['neg_triplet'].apply(is_bad_triplet)]\n",
    "\n",
    "df = remove_bad_triplets(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb47cb7d-7f06-40d1-82a4-4f2d2f901827",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_duplicated_rows_with_answers(df):\n",
    "    new_rows = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        # Determine the correct index based on negation flags\n",
    "        correct_index = 0 if row['subj_neg'] else (1 if row['verb_neg'] else 2)\n",
    "        \n",
    "        # Process for positive triplet\n",
    "        pos_triplet = parse_triplet(row['pos_triplet'])\n",
    "        neg_triplet = parse_triplet(row['neg_triplet'])\n",
    "        #print(neg_triplet)\n",
    "        # Create a row for the positive scenario\n",
    "        pos_row = row.copy()\n",
    "        #print(pos_triplet)\n",
    "        #print(row['pos_triplet'])\n",
    "        pos_row['correct_answer'] = pos_triplet[correct_index]\n",
    "        pos_row['url'] = row['pos_url']\n",
    "        new_rows.append(pos_row)\n",
    "        \n",
    "        # Create a row for the negative scenario\n",
    "        neg_row = row.copy()\n",
    "        neg_row['correct_answer'] = neg_triplet[correct_index]\n",
    "        neg_row['url'] = row['neg_url']\n",
    "        new_rows.append(neg_row)\n",
    "\n",
    "    # Create a new DataFrame\n",
    "    new_df = pd.DataFrame(new_rows).reset_index(drop=True)\n",
    "    return new_df\n",
    "\n",
    "# Call the function to duplicate rows and set 'correct_answer' and 'url' appropriately\n",
    "df = create_duplicated_rows_with_answers(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f3243d-a399-43bc-a890-7a4838157ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'url': 'clean_image_path'}, inplace=True)\n",
    "\n",
    "df['corrupt_image_path'] = df.apply(lambda row: row['neg_url'] if row['clean_image_path'] == row['pos_url'] else row['pos_url'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1c5645-f4a7-418d-aff5-7c124eec9ca2",
   "metadata": {},
   "source": [
    "## Now run make_chatgpt_prompts.py using the above dataset to generate \"gpt3.5_prompts.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f864e49-37c7-4b3f-bab6-53f4e3901fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = df[\"clean_prompt\"].to_list()\n",
    "chatgpt_prompts = pd.read_csv(\"gpt3.5_prompts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ed0167-f86c-4dc0-a325-04b20eab6a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "df  = df[~df[\"clean_prompt\"].str.contains('person')] #removing all prompts including \"person\", asking the model to choose between \"woman\" or \"person\" produces inconsistent results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a0f6b0-577f-4038-8dcf-f71657554ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df.drop_duplicates([\"clean_prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f15c45-3718-43e4-bb58-319002d50287",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new[\"clean_prompt_gpt\"] = chatgpt_prompts[\"GPT-3.5 Prompts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fc3a0c-8cc0-4b7f-9605-bd15e298e656",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this allows for both positive first and negative first answers\n",
    "#so this is exactly 14905 * 2\n",
    "df_with_duplicates = df_new[df_new[\"clean_prompt\"].isin(df[\"clean_prompt\"].values)].drop_duplicates([\"clean_prompt\", \"correct_answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ef0406-5fe3-4149-b681-281e091453f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_duplicates[\"clean_prompt\"] = df_with_duplicates[\"clean_prompt_gpt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4e7d23-d4e7-49e8-9bb2-09e6214b7b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_duplicates.to_csv(\"svo_prompts_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7de366-0523-4e57-9dda-b38d6b3f8540",
   "metadata": {},
   "source": [
    "# NOTE: since URLs can be a bit unpredictable in the SVO probes dataset, we provide code to save images directly.\n",
    "## This file will be approximately 30GB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a321a003-495a-4a6c-a73d-98ddec2d8a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import pickle\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, urls: list, transform: Callable[[Image.Image], torch.Tensor]) -> None:\n",
    "        self.urls = urls  # List of image URLs\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, i: int) -> Tuple[str, torch.Tensor]:\n",
    "        url = self.urls[i]\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)  # Fetch the image\n",
    "            image = Image.open(BytesIO(response.content)).convert(\"RGB\")  # Convert to a PIL image\n",
    "            image_tensor = self.transform(image)  # Apply the transform to the image\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image from {url}: {e}\")\n",
    "            image_tensor = torch.zeros(3, 224, 224)  # Fallback tensor in case of error\n",
    "        return url, image_tensor\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.urls)\n",
    "\n",
    "def save_image_tensors(image_tensors, filename=\"image_tensors.pkl\"):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(image_tensors, f)\n",
    "\n",
    "#This file is provided in the Github of SVO-Probes (https://github.com/google-deepmind/svo_probes)\n",
    "file_path = 'image_urls.txt'\n",
    "\n",
    "# Load URLs from a file\n",
    "good_urls = []\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        url = line.strip()\n",
    "        good_urls.append(url)\n",
    "\n",
    "# Define your transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
    "    transforms.ToTensor(),  # Convert images to tensor\n",
    "])\n",
    "\n",
    "# Create the dataset\n",
    "image_dataset = ImageDataset(urls=good_urls, transform=transform)\n",
    "image_data_loader = DataLoader(image_dataset, batch_size=4, num_workers=2, pin_memory=True)\n",
    "\n",
    "# Container for the image tensors\n",
    "image_tensors = []\n",
    "\n",
    "# Process the images and save them, including periodic saving\n",
    "batch_count = 0\n",
    "for urls, imgs in image_data_loader:\n",
    "    batch_count += 1\n",
    "    for url, img in zip(urls, imgs):\n",
    "        image_tensors.append((url, img))\n",
    "    if batch_count % 10 == 0:  # Save every 100 batches\n",
    "        save_image_tensors(image_tensors, f\"image_tensors_batch_{batch_count}.pkl\")\n",
    "        print(f\"Saved checkpoint at batch {batch_count}\")\n",
    "\n",
    "# Save the final set of image tensors to a file\n",
    "save_image_tensors(image_tensors, \"image_tensors_final.pkl\")\n",
    "print(\"Image tensors saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
