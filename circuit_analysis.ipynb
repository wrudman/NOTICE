{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8504b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "from rapidfuzz import process, fuzz\n",
    "import pandas as pd\n",
    "from transformers import BlipProcessor\n",
    "from models_patching import ModifiedBlipForQuestionAnswering\n",
    "from plotting_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8d5182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_matches(row):\n",
    "    sentence = row['clean_prompt']\n",
    "    triplets = clean_and_combine_triplets(row['pos_triplet'], row['neg_triplet'])\n",
    "    inputs = processor.tokenizer(sentence, return_tensors=\"pt\")\n",
    "    tokens = processor.tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "\n",
    "    matches = {'[CLS]': 0, '[SEP]': tokens.index('[SEP]')}\n",
    "    # Exact matches first\n",
    "    for token in tokens:\n",
    "        clean_token = token.replace('##', '')\n",
    "        for triplet in list(triplets):\n",
    "            if clean_token == triplet:\n",
    "                matches[triplet] = tokens.index(token)\n",
    "                triplets.remove(triplet)  # Remove matched triplet to avoid re-matching\n",
    "\n",
    "    # Special tokens and key words\n",
    "    special_tokens = {'?': None}\n",
    "    for key in special_tokens.keys():\n",
    "        if key in tokens:\n",
    "            special_tokens[key] = tokens.index(key)\n",
    "    matches.update(special_tokens)\n",
    "\n",
    "    # Similarity matching for remaining triplets\n",
    "    for triplet in triplets:\n",
    "        options = [(t, i) for i, t in enumerate(tokens) if i not in matches.values()]\n",
    "        if not options:\n",
    "            break\n",
    "        option_tokens, _ = zip(*options)\n",
    "        clean_options = [t.replace('##', '') for t in option_tokens]\n",
    "        best_match = process.extractOne(triplet, clean_options, scorer=fuzz.WRatio)\n",
    "        if best_match and best_match[1] > 55:\n",
    "            best_match_token = best_match[0]\n",
    "            best_match_index = next(i for i, t in enumerate(tokens) if t.replace('##', '') == best_match_token and i not in matches.values())\n",
    "            matches[triplet] = best_match_index\n",
    "            \n",
    "    return matches\n",
    "\n",
    "\n",
    "def extract_selected_one_score(row):\n",
    "    scores = row['temp_list'][\"scores\"]\n",
    "    correct_answer = row['correct_answer']\n",
    "    sentence = row[\"clean_prompt\"]\n",
    "    inputs = processor.tokenizer(sentence, return_tensors=\"pt\")\n",
    "    tokens = processor.tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "\n",
    "    correct_answer_tokens = processor.tokenizer.tokenize(correct_answer)\n",
    "    \n",
    "    index_to_extract = None\n",
    "    for i in range(len(tokens) - len(correct_answer_tokens) + 1):\n",
    "        if tokens[i:i+len(correct_answer_tokens)] == correct_answer_tokens:\n",
    "            index_to_extract = i\n",
    "            break\n",
    "    \n",
    "    if index_to_extract is not None:\n",
    "        selected_scores = scores[index_to_extract]\n",
    "    else:\n",
    "        print(\"no correct answer found\")\n",
    "        selected_scores = None \n",
    "    # 0 index for restoration probability, 2 for logit difference. \n",
    "    return selected_scores[:, 0] if selected_scores is not None else None\n",
    "\n",
    "\n",
    "def get_coordinates(flattened_indices, num_rows=12, num_cols=12):\n",
    "    coordinates = []\n",
    "    for index in flattened_indices:\n",
    "        row = index // num_cols\n",
    "        col = index % num_cols\n",
    "        coordinates.append((row, col))\n",
    "    return coordinates\n",
    "\n",
    "\n",
    "# 2xstd logit difference\n",
    "def get_important_heads(mrr, threshold):\n",
    "    # Flatten the array\n",
    "    \n",
    "    print(mrr.shape)\n",
    "    flat_arr = abs(mrr.flatten())\n",
    "    # Get the indices of the top n elements by magnitude\n",
    "    top_n_indices_flat = np.where(flat_arr >= threshold)[0]\n",
    "    \n",
    "    return get_coordinates(top_n_indices_flat)\n",
    "\n",
    "def get_circuits(scores):\n",
    "    num_samples = scores.shape[0]\n",
    "    avg_by_head = np.mean(abs(np.reshape(scores, (num_samples,-1))), axis=0)\n",
    "    all_avg = np.mean(avg_by_head)\n",
    "    threshold = 2*np.std(avg_by_head) + all_avg\n",
    "    return set(get_important_heads(avg_by_head, threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fb9eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the list to store scores for all samples and heads\n",
    "all_scores = []\n",
    "\n",
    "# Load the DataFrame once\n",
    "df_correct = pd.read_csv(df_filepath)\n",
    "\n",
    "task='svo_probes'\n",
    "block_name'text_encoder'\n",
    "kind='crossattention_block'\n",
    "mode='image'\n",
    "\n",
    "# Loop over 12 attention heads\n",
    "for attn_head in range(12):\n",
    "    with open(f'BLIP_temp_list_{task}_{mode}_corruption_{block_name}_{kind}_head_{attn_head}_{len(df_correct)}.pkl', 'rb') as file:\n",
    "        loaded_temp_list = pickle.load(file)\n",
    "    \n",
    "    df_correct[\"temp_list\"] = loaded_temp_list\n",
    "    df_correct['selected_scores'] = df_correct.apply(extract_selected_one_score_emotions, axis=1)\n",
    "    \n",
    "    selected_scores = np.array(df_correct['selected_scores'].tolist())\n",
    "    all_scores.append(selected_scores)\n",
    "\n",
    "# Convert the list of selected scores to a numpy array\n",
    "all_scores = np.stack(all_scores, axis=1) \n",
    "all_scores = np.nan_to_num(all_scores, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "all_scores = all_scores.transpose(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27133bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_circuits(all_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a91ddb6",
   "metadata": {},
   "source": [
    "# VISUALING CROSS ATTENTION FOR IMPORTANT HEADS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590b76df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from rapidfuzz import process, fuzz\n",
    "import pandas as pd\n",
    "from transformers import BlipProcessor\n",
    "from models_patching import ModifiedBlipForQuestionAnswering\n",
    "from plotting_utils import *\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from scipy.ndimage import zoom\n",
    "import seaborn as sns\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import io \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ModifiedBlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "model.to(device)\n",
    "\n",
    "processor=BlipProcessor.from_pretrained('Salesforce/blip-vqa-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740ad866",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def get_avg_attention(outputs, toi_index):\n",
    "    avg = []\n",
    "    cls_attention = defaultdict(int)\n",
    "    for layer in range(12):\n",
    "        for head in range(12):\n",
    "            # Compute average attention matrix.                       #index of the tuple  #batch_index #j = attention head index\n",
    "            cross_attention_vals = outputs['cross_attentions'][layer][0][head][toi_index]\n",
    "            cross_attention_vals = cross_attention_vals.detach().cpu().numpy()[1:]\n",
    "            cls_attention[str(layer) + \"_\" + str(head)] = cross_attention_vals[0]\n",
    "            avg.append(cross_attention_vals)\n",
    "\n",
    "    return np.mean(np.vstack(avg), axis=0), cls_attention\n",
    "\n",
    "# Function to create a hook that collects the states\n",
    "def create_hook(layer_outputs, layer_num):\n",
    "    def hook(module, input, output):\n",
    "        layer_outputs[layer_num] = output[1]\n",
    "    return hook\n",
    "\n",
    "# Function to register hooks to the specified layers\n",
    "def register_hooks(model):\n",
    "    layer_outputs = {}\n",
    "    for num in range(12):  # from 0 to 12 inclusive\n",
    "        layer_name = f\"text_encoder.encoder.layer.{num}.crossattention.self\"\n",
    "        layer = dict(model.named_modules())[layer_name]\n",
    "        layer.register_forward_hook(create_hook(layer_outputs, num))\n",
    "    return layer_outputs\n",
    "\n",
    "layer_outputs = register_hooks(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93eb4c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_avg_crossattention(outputs, pixel_values, avg_attention, layer, head, toi_index):\n",
    "        # Compute average attention matrix.                       #index of the tuple  #batch_index #j = attention head index\n",
    "        # layer, batch_index which is 0, attention head, correct_answer_token. \n",
    "        average_attention_matrix_vals = avg_attention #* mask\n",
    "\n",
    "        # Assuming you have the base image as pixel_values\n",
    "        pixel_values = pixel_values.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "        # Normalize the pixel values to the range [0, 255]\n",
    "        pixel_values = (pixel_values - pixel_values.min()) / (pixel_values.max() - pixel_values.min()) * 255\n",
    "        pixel_values = pixel_values.astype(np.uint8)\n",
    "\n",
    "        # Reshape to a column vector (576x1)\n",
    "        attention_vector = average_attention_matrix_vals.reshape(-1, 1)\n",
    "\n",
    "        # Assuming the image is divided into a 24x24 grid of patches of size 16x16\n",
    "        grid_size = 24\n",
    "        patch_size = 16\n",
    "\n",
    "        # Resize attention_vector to match the number of patches\n",
    "        n_patches = grid_size * grid_size\n",
    "        resized_attention_vector = zoom(attention_vector, (n_patches / len(attention_vector), 1)).reshape(grid_size, grid_size)\n",
    "\n",
    "        # Normalize attention values\n",
    "        patch_attention_normalized = (resized_attention_vector - np.min(resized_attention_vector)) / (np.max(resized_attention_vector) - np.min(resized_attention_vector))\n",
    "\n",
    "        # Resize the patch attention heatmap to match the base image dimensions\n",
    "        patch_attention_resized = zoom(patch_attention_normalized, (pixel_values.shape[0] / grid_size, pixel_values.shape[1] / grid_size))\n",
    "\n",
    "        # Create a figure and a single subplot with high DPI\n",
    "        fig, ax = plt.subplots(figsize=(8, 8), dpi=200)\n",
    "\n",
    "        # Display the base image\n",
    "        ax.imshow(pixel_values, aspect='auto')\n",
    "\n",
    "        # Create the attention map overlay with the 'seismic' colormap\n",
    "        sns.heatmap(patch_attention_resized, cmap='seismic', alpha=0.5, ax=ax, zorder=2, cbar=True)\n",
    "\n",
    "        # Remove the axis labels for a cleaner look\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "        # Add a title\n",
    "        ax.set_title(f'Avg. Cross-Attention',fontsize=28)\n",
    "        #file_path = f'catt_visualization_samples/Minus_AVG_AttentionOverlay_L{i}_H{j}.png'\n",
    "        #plt.savefig(file_path)\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175e1eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through different attention matrices\n",
    "def visualize_crossattention(outputs, pixel_values, avg_attention, layer, head, toi_index):\n",
    "        # Compute average attention matrix.                       #index of the tuple  #batch_index #j = attention head index\n",
    "        # layer, batch_index which is 0, attention head, correct_answer_token. \n",
    "        average_attention_matrix_vals = outputs[layer][0][head][toi_index]\n",
    "        average_attention_matrix_vals = average_attention_matrix_vals.detach().cpu().numpy()[1:] - avg_attention #* mask\n",
    "\n",
    "        # Assuming you have the base image as pixel_values\n",
    "        pixel_values = pixel_values.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "        # Normalize the pixel values to the range [0, 255]\n",
    "        pixel_values = (pixel_values - pixel_values.min()) / (pixel_values.max() - pixel_values.min()) * 255\n",
    "        pixel_values = pixel_values.astype(np.uint8)\n",
    "\n",
    "        # Reshape to a column vector (576x1)\n",
    "        attention_vector = average_attention_matrix_vals.reshape(-1, 1)\n",
    "\n",
    "        # Assuming the image is divided into a 24x24 grid of patches of size 16x16\n",
    "        grid_size = 24\n",
    "        patch_size = 16\n",
    "\n",
    "        # Resize attention_vector to match the number of patches\n",
    "        n_patches = grid_size * grid_size\n",
    "        resized_attention_vector = zoom(attention_vector, (n_patches / len(attention_vector), 1)).reshape(grid_size, grid_size)\n",
    "\n",
    "        # Normalize attention values\n",
    "        patch_attention_normalized = (resized_attention_vector - np.min(resized_attention_vector)) / (np.max(resized_attention_vector) - np.min(resized_attention_vector))\n",
    "\n",
    "        # Resize the patch attention heatmap to match the base image dimensions\n",
    "        patch_attention_resized = zoom(patch_attention_normalized, (pixel_values.shape[0] / grid_size, pixel_values.shape[1] / grid_size))\n",
    "\n",
    "        # Create a figure and a single subplot with high DPI\n",
    "        fig, ax = plt.subplots(figsize=(8, 8), dpi=200)\n",
    "\n",
    "        # Display the base image\n",
    "        ax.imshow(pixel_values, aspect='auto')\n",
    "\n",
    "        # Create the attention map overlay with the 'seismic' colormap\n",
    "        sns.heatmap(patch_attention_resized, cmap='seismic', alpha=0.5, ax=ax, zorder=2, cbar=True)\n",
    "\n",
    "        # Remove the axis labels for a cleaner look\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "        # Add a title\n",
    "        ax.set_title(f'L.{layer} H.{head}',fontsize=34)\n",
    "        #file_path = f'catt_visualization_samples/Minus_AVG_AttentionOverlay_L{i}_H{j}.png'\n",
    "        #plt.savefig(file_path)\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc7f91e",
   "metadata": {},
   "source": [
    "### UNIVERSAL HEADS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acc4a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "universal_heads = {(5,3),(3,0),(0,11)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11d145d",
   "metadata": {},
   "source": [
    "### Plot the attention pattern over image patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a685c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: IF looking at MIT STATES, use toi_index=3 to get the object. Otherswise it will find the correct adj. \n",
    "def get_toi_index(row):\n",
    "    correct_answer = row['correct_answer']\n",
    "    sentence = row[\"clean_prompt\"]\n",
    "\n",
    "    inputs = processor.tokenizer(sentence, return_tensors=\"pt\")\n",
    "    tokens = processor.tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    correct_answer_tokens = processor.tokenizer.tokenize(correct_answer)\n",
    "    \n",
    "    index_to_extract = None\n",
    "    for i in range(len(tokens) - len(correct_answer_tokens) + 1):\n",
    "        if tokens[i:i+len(correct_answer_tokens)] == correct_answer_tokens:\n",
    "            print(\"TOI\", tokens[i:i+len(correct_answer_tokens)])\n",
    "            return i\n",
    "    return f\"NO MATCH FOUND for {correct_answer} in {sentence}\" \n",
    "\n",
    "def plot_sample(df_sample)\n",
    "    for idx, row in df_sample.iterrows():\n",
    "        image_path = row[\"correct_image_path\"]\n",
    "        image = Image.open(image_path)\n",
    "        prompt = row['clean_prompt']\n",
    "        inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n",
    "        outputs = model(**inputs, output_attentions=True)\n",
    "\n",
    "        toi_index = get_toi_index(row)\n",
    "        avg_attention, cls_attention = get_avg_attention(outputs, toi_index)\n",
    "\n",
    "        for universal_head in universal_heads: #universal_heads:\n",
    "            layer, head = universal_head\n",
    "\n",
    "            visualize_crossattention(layer_outputs, inputs.pixel_values, avg_attention, layer, head, toi_index)\n",
    "        visualize_avg_crossattention(layer_outputs, inputs.pixel_values, avg_attention, layer, head, toi_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
